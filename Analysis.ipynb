{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TabishHussainRaza/NetworkAnalysis/blob/main/Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "struZAGw66hN"
      },
      "source": [
        "# **Network Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EScIJh2In7cX",
        "outputId": "5e67573c-c3a0-4009-8fae-09efbf34f816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.35 langchain-text-splitters-0.2.2 langsmith-0.1.106 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgjrWmTsoKb-",
        "outputId": "100e7b9c-a734-4f98-8b43-46e226a59c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.35)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.106)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0FrLywUpLR8",
        "outputId": "8e7640c8-5356-47c5-9498-217ce291d952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m286.7/295.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYFh5RLws9ln",
        "outputId": "e4016534-f4d9-434c-fd19-6453c99c042c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSMdn7FM9fAU"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Function to load and save content\n",
        "def save_web_content(url_list, category_name):\n",
        "    for i, url in enumerate(url_list):\n",
        "        loader = WebBaseLoader(url)\n",
        "        docs = loader.load()\n",
        "\n",
        "        # Save the content of the first document (assuming a single doc is loaded)\n",
        "        content = docs[0].page_content if docs else \"No content loaded\"\n",
        "\n",
        "        # Save to a text file with a specified name pattern\n",
        "        file_name = f\"{category_name}_{i + 1}.txt\"\n",
        "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "        print(f\"Saved content to {file_name}\")\n",
        "\n",
        "\n",
        "france_public_opinion = [\n",
        "    \"https://www.euractiv.com/section/electricity/news/most-french-want-government-to-speed-up-renewable-nuclear-development/\",\n",
        "    \"https://www.solarplaza.com/resource/12197/way-forward-solar-france/\",\n",
        "    \"https://www.orano.group/en/unpacking-nuclear/according-to-a-bva-study-for-orano-most-french-people-think-nuclear-energy-is-an-asset-for-france-s-energy-independence\"\n",
        "]\n",
        "\n",
        "save_web_content(france_public_opinion, \"france_public_opinion\")\n",
        "\n",
        "france_stakeholder_info = [\n",
        "    \"https://www.agenda-2030.fr/en/agenda-2030/france/article/mobilized-stakeholders\"\n",
        "]\n",
        "\n",
        "save_web_content(france_stakeholder_info, \"france_stakeholder_info\")\n",
        "\n",
        "\n",
        "aus_public_opinion = [\n",
        "    \"https://world-nuclear-news.org/Articles/National-poll-shows-shift-in-Australian-nuclear-op\",\n",
        "    \"https://poll.lowyinstitute.org/charts/nuclear-power-in-australia/\",\n",
        "    \"https://poll.lowyinstitute.org/charts/australia-using-nuclear-power-to-generate-energy/\",\n",
        "    \"https://australiainstitute.org.au/report/polling-willingness-to-pay-for-nuclear/\",\n",
        "    \"https://www.tenmenelectrical.com/solar-power-is-australians-most-preferred-energy-source/\",\n",
        "    \"https://www.theguardian.com/environment/2014/dec/08/solar-wind-energy-sources-huge-majority-australians-poll-shows\"\n",
        "]\n",
        "\n",
        "save_web_content(aus_public_opinion, \"aus_public_opinion\")\n",
        "\n",
        "aus_stakeholder_info = [\n",
        "    \"https://www.helixos.co/post/stakeholder-engagement-on-nuclear-energy-in-australia\\\",\n",
        "    \"https://createdigital.org.au/30-billion-australia-asia-powerlink-project\\\",\n",
        "    \"https://www.corrs.com.au/insights/legal-and-social-licence-considerations-for-nuclear-energy-in-australia\\\"\n",
        "]\n",
        "\n",
        "save_web_content(aus_stakeholder_info, \"aus_stakeholder_info\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFjgUm9uA2TJ"
      },
      "outputs": [],
      "source": [
        "def load_document_by_index(file_paths, index):\n",
        "    \"\"\"\n",
        "    Loads the content of a document based on the provided index.\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): A list of file paths (e.g., \"/content/aus_public_opinion_1.txt\").\n",
        "        index (int): The index of the document to load from the list.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the file as a string.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(file_paths):\n",
        "        return \"Invalid index. Please provide a valid index within the range.\"\n",
        "\n",
        "    file_path = file_paths[index]\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error loading document: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_paths_australia = [\n",
        "    # Australia Public Opinion\n",
        "    \"/content/aus_public_opinion_1.txt\",\n",
        "    \"/content/aus_public_opinion_2.txt\",\n",
        "    \"/content/aus_public_opinion_3.txt\",\n",
        "    \"/content/aus_public_opinion_4.txt\",\n",
        "    \"/content/aus_public_opinion_5.txt\",\n",
        "    \"/content/aus_public_opinion_6.txt\",\n",
        "\n",
        "    # Australia Stakeholder Info\n",
        "    \"/content/aus_stakeholder_info_1.txt\",\n",
        "    \"/content/aus_stakeholder_info_2.txt\",\n",
        "    \"/content/aus_stakeholder_info_3.txt\",\n",
        "\n",
        "\n",
        "    # Australia Policy Docuemnts\n",
        "    \"/content/drive/Data/Australia2023EnergyPolicyReview_extracted_paragraphs.txt\",\n",
        "    \"/content/drive/Data/AustraliaPublicPolicy.txt\",\n",
        "]\n",
        "\n",
        "document_paths_france = [\n",
        "    # France Public Opinion\n",
        "    \"/content/france_public_opinion_2.txt\",\n",
        "    \"/content/france_public_opinion_3.txt\",\n",
        "\n",
        "    # France Stakeholder Info\n",
        "    \"/content/france_stakeholder_info_1.txt\",\n",
        "\n",
        "    # France Policy Docuemnts\n",
        "    \"/content/drive/Data/Energy_Policy_France_2016_Review_extracted_paragraphs.txt\",\n",
        "]\n",
        "\n",
        "document_paths_singapore = [\n",
        "    # Singapore Public Opinion\n",
        "    \"/content/singapore_public_opinion_2.txt\",\n",
        "    \"/content/singapore_public_opinion_3.txt\",\n",
        "\n",
        "    # Singapore Stakeholder Info\n",
        "    \"/content/singapore_stakeholder_info_1.txt\",\n",
        "\n",
        "    # Singapore Policy Documents\n",
        "    \"/content/drive/Data/SingaporePublicPolicy.txt\",\n",
        "    \"/content/drive/Data/singapore_nr_extracted_paragraphs.txt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob  # Import TextBlob for sentiment analysis\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 1: Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by normalizing, removing stop words, and cleaning.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    doc = nlp(text)\n",
        "    cleaned_text = ' '.join([token.text for token in doc])\n",
        "    return cleaned_text\n",
        "\n",
        "# Step 2: Entity Recognition\n",
        "def extract_entities(doc):\n",
        "    \"\"\"\n",
        "    Extract and filter entities using spaCy's NER.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'GPE', 'PERSON', 'NORP', 'DATE'] and len(ent.text.strip()) > 2:\n",
        "            entities.append((ent.text.strip(), ent.label_))\n",
        "    return entities\n",
        "\n",
        "# Step 3: Dependency Parsing and Step 4: Relationship Extraction\n",
        "def extract_relationships(doc):\n",
        "    \"\"\"\n",
        "    Extract meaningful relationships between entities based on dependency parsing.\n",
        "    \"\"\"\n",
        "    relationships = []\n",
        "    pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves',\n",
        "                'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
        "                'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'this', 'that', 'these', 'those'}\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        subject = None\n",
        "        object_ = None\n",
        "        verb = None\n",
        "\n",
        "        for token in sent:\n",
        "            if token.dep_ in ['nsubj', 'nsubjpass']:\n",
        "                if token.text.lower() not in pronouns:\n",
        "                    subject = token.text\n",
        "                    verb = token.head.text\n",
        "            if token.dep_ in ['dobj', 'pobj', 'attr']:\n",
        "                if token.text.lower() not in pronouns:\n",
        "                    object_ = token.text\n",
        "\n",
        "            if subject and object_ and verb:\n",
        "                relationships.append((subject, verb, object_))\n",
        "                subject, object_, verb = None, None, None\n",
        "\n",
        "    return relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Sentiment Analysis\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Perform sentiment analysis on the preprocessed text.\n",
        "    \"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity  # Polarity ranges from -1 (negative) to 1 (positive)\n",
        "    return sentiment\n",
        "\n",
        "# Step 5: Ensure all entities are included in the graph\n",
        "def ensure_entity_connections(entities, relationships):\n",
        "    \"\"\"\n",
        "    Ensure all entities extracted have at least one relationship.\n",
        "    \"\"\"\n",
        "    connected_entities = set([source for source, _, _ in relationships] + [target for _, _, target in relationships])\n",
        "    final_entities = [entity for entity in entities if entity[0] in connected_entities]\n",
        "    return final_entities, relationships\n",
        "\n",
        "# Step 6: Visualization using displaCy\n",
        "def visualize(doc):\n",
        "    \"\"\"\n",
        "    Visualize entities and dependencies using displaCy.\n",
        "    \"\"\"\n",
        "    # Visualize Named Entities\n",
        "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "\n",
        "    # Visualize Dependency Parsing\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 100})\n",
        "\n",
        "# Function to load document by index\n",
        "def load_document_by_index(file_paths, index):\n",
        "    \"\"\"\n",
        "    Loads the content of a document based on the provided index.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(file_paths):\n",
        "        return \"Invalid index. Please provide a valid index within the range.\"\n",
        "\n",
        "    file_path = file_paths[index]\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error loading document: {e}\"\n",
        "\n",
        "# Main processing loop\n",
        "if _name_ == \"_main_\":\n",
        "\n",
        "    # Initialize an empty graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Initialize empty DataFrames for entities, relationships, and sentiments\n",
        "    all_entities_df = pd.DataFrame(columns=['Entity', 'Type'])\n",
        "    all_relationships_df = pd.DataFrame(columns=['Source', 'Relation', 'Target'])\n",
        "    all_sentiments_df = pd.DataFrame(columns=['Document', 'Sentiment'])\n",
        "\n",
        "    # Iterate over each document in the list\n",
        "    for index, path in enumerate(document_paths_australia):\n",
        "        text = load_document_by_index(document_paths_australia, index)\n",
        "\n",
        "        # Step 1: Preprocess the text\n",
        "        preprocessed_text = preprocess_text(text)\n",
        "\n",
        "        # Step 2: Process the document with spaCy NLP pipeline\n",
        "        doc = nlp(preprocessed_text)\n",
        "\n",
        "        # Step 3: Extract entities\n",
        "        entities = extract_entities(doc)\n",
        "\n",
        "        # Step 4: Extract relationships\n",
        "        relationships = extract_relationships(doc)\n",
        "\n",
        "        # Perform Sentiment Analysis only for \"public opinion\" documents\n",
        "        if \"public_opinion\" in path:\n",
        "            sentiment = analyze_sentiment(preprocessed_text)\n",
        "            print(f\"Document: {path}, Sentiment Polarity: {sentiment}\")\n",
        "\n",
        "        # Step 5: Ensure all entities are connected by at least one relationship\n",
        "        final_entities, final_relationships = ensure_entity_connections(entities, relationships)\n",
        "\n",
        "        # Step 6: Visualize the results\n",
        "        visualize(doc)\n",
        "\n",
        "        # Append entities and relationships to the overall DataFrame\n",
        "        all_entities_df = pd.concat([all_entities_df, pd.DataFrame(final_entities, columns=['Entity', 'Type'])], ignore_index=True)\n",
        "        all_relationships_df = pd.concat([all_relationships_df, pd.DataFrame(final_relationships, columns=['Source', 'Relation', 'Target'])], ignore_index=True)\n",
        "\n",
        "        # Add nodes and edges to the graph\n",
        "        for entity, entity_type in final_entities:\n",
        "            G.add_node(entity, type=entity_type)\n",
        "\n",
        "        for source, relation, target in final_relationships:\n",
        "            G.add_edge(source, target, relation=relation)\n",
        "\n",
        "        # Draw the graph after processing each document\n",
        "        plt.figure(figsize=(12, 12))\n",
        "        nx.draw(G, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', edge_color='gray')\n",
        "        plt.title(f\"Graph after processing document {index + 1}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Remove duplicate entities and relationships\n",
        "    all_entities_df = all_entities_df.drop_duplicates()\n",
        "    all_relationships_df = all_relationships_df.drop_duplicates()\n",
        "\n",
        "    # Save the final deduplicated entities, relationships, and sentiments\n",
        "    all_entities_df.to_csv('final_entities_australia.csv', index=False)\n",
        "    all_relationships_df.to_csv('final_relationships_australia.csv', index=False)\n",
        "\n",
        "    # Visualize the final combined graph\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    nx.draw(G, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', edge_color='gray')\n",
        "    plt.title(\"Final Combined Graph\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
